{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial on training a HTS-AT model for audio classification on the ESC-50 Dataset\n",
    "\n",
    "Referece: \n",
    "\n",
    "[HTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection, ICASSP 2022](https://arxiv.org/abs/2202.00874)\n",
    "\n",
    "Following the HTS-AT's paper, in this tutorial, we would show how to use the HST-AT in the training of the ESC-50 Dataset.\n",
    "\n",
    "The [ESC-50 dataset](https://github.com/karolpiczak/ESC-50) is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories\n",
    "\n",
    "Before running this tutorial, please make sure that you install the below packages by following steps:\n",
    "\n",
    "1. download [the codebase](https://github.com/RetroCirce/HTS-Audio-Transformer), and put this tutorial notebook inside the codebase folder.\n",
    "\n",
    "2. In the github code folder:\n",
    "\n",
    "    > pip install -r requirements.txt\n",
    "\n",
    "3. We do not include the installation of PyTorch in the requirment, since different machines require different vereions of CUDA and Toolkits. So make sure you install the PyTorch from [the official guidance](https://pytorch.org/).\n",
    "\n",
    "4. Install the 'SOX' and the 'ffmpeg', we recommend that you run this code in Linux inside the Conda environment. In that, you can install them by:\n",
    "\n",
    "    > sudo apt install sox\n",
    "    \n",
    "    > conda install -c conda-forge ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-03-27T14:27:37.018190500Z"
    }
   },
   "outputs": [],
   "source": [
    "# import basic packages\n",
    "import os\n",
    "import numpy as np\n",
    "import wget\n",
    "import sys\n",
    "import gdown\n",
    "import zipfile\n",
    "import librosa\n",
    "# in the notebook, we only can use one GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Build the workspace and download the needed files\n",
    "\n",
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "workspace = \"./workspace_ADS_v2\"\n",
    "dataset_path = os.path.join(workspace, \"mfg_robot\")\n",
    "checkpoint_path = os.path.join(workspace, \"ckpt\")\n",
    "mfg_raw_path = os.path.join(dataset_path, 'raw')\n",
    "\n",
    "\n",
    "create_path(workspace)\n",
    "create_path(dataset_path)\n",
    "create_path(checkpoint_path)\n",
    "create_path(mfg_raw_path)\n",
    "\n",
    "\n",
    "# # download the esc-50 dataset\n",
    "# \n",
    "# if not os.path.exists(os.path.join(dataset_path, 'ESC-50-master.zip')):\n",
    "#     print(\"-------------Downloading ESC-50 Dataset-------------\")\n",
    "#     wget.download('https://github.com/karoldvl/ESC-50/archive/master.zip', out=dataset_path)\n",
    "#     with zipfile.ZipFile(os.path.join(dataset_path, 'ESC-50-master.zip'), 'r') as zip_ref:\n",
    "#         zip_ref.extractall(esc_raw_path)\n",
    "#     print(\"-------------Success-------------\")\n",
    "# \n",
    "# if not os.path.exists(os.path.join(checkpoint_path,'htsat_audioset_pretrain.ckpt')):\n",
    "#     gdown.download(id='1OK8a5XuMVLyeVKF117L8pfxeZYdfSDZv', output=os.path.join(checkpoint_path,'htsat_audioset_pretrain.ckpt'))\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Resample ESC-50-------------\n",
      "-------------Resample Success-------------\n"
     ]
    }
   ],
   "source": [
    "# Process Manufacturing Dataset – Resampling Audio Files\n",
    "\n",
    "audio_path = os.path.join(mfg_raw_path, 'MFG-master', 'audio')\n",
    "resample_path = os.path.join(dataset_path, 'resample')\n",
    "savedata_path = os.path.join(dataset_path, 'mfg-data.npy')\n",
    "create_path(resample_path)\n",
    "\n",
    "audio_list = os.listdir(audio_path)\n",
    "\n",
    "print(\"-------------Resample ESC-50-------------\")\n",
    "for f in audio_list:\n",
    "    full_f = os.path.join(audio_path, f)\n",
    "    resample_f = os.path.join(resample_path, f)\n",
    "    if not os.path.exists(resample_f):\n",
    "        os.system('sox -V1 ' + full_f + ' -r 32000 ' + resample_f)\n",
    "print(\"-------------Resample Success-------------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T14:23:47.659713400Z",
     "start_time": "2025-03-27T14:23:47.645188500Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Build Dataset-------------\n",
      "-------------Build Dataset Success-------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Paths\n",
    "meta_path = os.path.join(mfg_raw_path, 'MFG-master\\meta\\mfg.csv')  # Adjust this path if needed\n",
    "meta = np.loadtxt(meta_path, delimiter=',', dtype='str', skiprows=1)\n",
    "\n",
    "print(\"-------------Build Dataset-------------\")\n",
    "output_dict = [[] for _ in range(5)]  # Assuming 5 folds, still okay if we only use fold 1\n",
    "\n",
    "for label in meta:\n",
    "    name = label[0]\n",
    "    fold = int(label[1])\n",
    "    target = int(label[2])\n",
    "    \n",
    "    #y, sr = librosa.load(os.path.join(resample_path, name), sr=None)\n",
    "    \n",
    "    # Preserve the orinal multi-channel structure for the sensor data\n",
    "    y, sr = librosa.load(os.path.join(resample_path, name), sr=None, mono=False)\n",
    "\n",
    "    output_dict[fold - 1].append({\n",
    "        \"name\": name,\n",
    "        \"target\": target,\n",
    "        \"waveform\": y\n",
    "    })\n",
    "\n",
    "np.save(savedata_path, np.array(output_dict, dtype=object))\n",
    "\n",
    "print(\"-------------Build Dataset Success-------------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T14:23:48.860859800Z",
     "start_time": "2025-03-27T14:23:48.567302800Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:23:50.187125900Z",
     "start_time": "2025-03-27T14:23:50.152151400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model package\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "\n",
    "from utils import create_folder, dump_config, process_idc\n",
    "import mfg_config as config\n",
    "from sed_model import SEDWrapper, Ensemble_SEDWrapper\n",
    "from data_generator_mfg import MFG_Dataset\n",
    "from model.htsat_mfg_v_2_0 import HTSAT_Swin_Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "# New data preparation class\n",
    "class data_prep(pl.LightningDataModule):\n",
    "    def __init__(self, dataset, config, device_num):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset  # Store only a reference\n",
    "        self.config = config\n",
    "        self.device_num = device_num\n",
    "        self.train_dataset = None  # Placeholder, will be initialized later\n",
    "        self.eval_dataset = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"This method is called inside Lightning, and it ensures datasets are created properly.\"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = MFG_Dataset(\n",
    "                dataset=self.dataset,\n",
    "                config=self.config,\n",
    "                eval_mode=False\n",
    "            )\n",
    "            self.eval_dataset = MFG_Dataset(\n",
    "                dataset=self.dataset,\n",
    "                config=self.config,\n",
    "                eval_mode=True\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_sampler = DistributedSampler(self.train_dataset, shuffle=False) if self.device_num > 1 else None\n",
    "        return DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            num_workers=self.config.num_workers,\n",
    "            batch_size=self.config.batch_size // max(1, self.device_num),\n",
    "            shuffle=False,\n",
    "            sampler=train_sampler\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        eval_sampler = DistributedSampler(self.eval_dataset, shuffle=False) if self.device_num > 1 else None\n",
    "        return DataLoader(\n",
    "            dataset=self.eval_dataset,\n",
    "            num_workers=self.config.num_workers,\n",
    "            batch_size=self.config.batch_size // max(1, self.device_num),\n",
    "            shuffle=False,\n",
    "            sampler=eval_sampler\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_sampler = DistributedSampler(self.eval_dataset, shuffle=False) if self.device_num > 1 else None\n",
    "        return DataLoader(\n",
    "            dataset=self.eval_dataset,\n",
    "            num_workers=self.config.num_workers,\n",
    "            batch_size=self.config.batch_size // max(1, self.device_num),\n",
    "            shuffle=False,\n",
    "            sampler=test_sampler\n",
    "        )\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        \"\"\"Removes unpicklable attributes before multiprocessing starts\"\"\"\n",
    "        for attr in [\"trainer\", \"prepare_data\", \"setup\", \"teardown\"]:\n",
    "            if hasattr(self, attr):\n",
    "                delattr(self, attr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T14:23:51.123423200Z",
     "start_time": "2025-03-27T14:23:51.094364700Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:26:59.003164800Z",
     "start_time": "2025-03-27T14:26:58.952164800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_num: 1\n",
      "each batch size: 64\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'workspace/mfg_robot\\\\mfg-data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdevice_num:\u001B[39m\u001B[38;5;124m\"\u001B[39m, device_num)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meach batch size:\u001B[39m\u001B[38;5;124m\"\u001B[39m, config\u001B[38;5;241m.\u001B[39mbatch_size \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m device_num)\n\u001B[1;32m----> 6\u001B[0m full_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmfg-data.npy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_pickle\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# set exp folder\u001B[39;00m\n\u001B[0;32m      9\u001B[0m exp_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(config\u001B[38;5;241m.\u001B[39mworkspace, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\"\u001B[39m, config\u001B[38;5;241m.\u001B[39mexp_name)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\numpy\\lib\\npyio.py:407\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001B[0m\n\u001B[0;32m    405\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    406\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 407\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos_fspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    408\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    410\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'workspace/mfg_robot\\\\mfg-data.npy'"
     ]
    }
   ],
   "source": [
    "# Set the workspace\n",
    "device_num = torch.cuda.device_count()\n",
    "print(\"device_num:\", device_num)\n",
    "print(\"each batch size:\", config.batch_size // device_num)\n",
    "\n",
    "full_dataset = np.load(os.path.join(config.dataset_path, \"mfg-data.npy\"), allow_pickle = True)\n",
    "\n",
    "# set exp folder\n",
    "exp_dir = os.path.join(config.workspace, \"results\", config.exp_name)\n",
    "checkpoint_dir = os.path.join(config.workspace, \"results\", config.exp_name, \"checkpoint\")\n",
    "if not config.debug:\n",
    "    create_folder(os.path.join(config.workspace, \"results\"))\n",
    "    create_folder(exp_dir)\n",
    "    create_folder(checkpoint_dir)\n",
    "    dump_config(config, os.path.join(exp_dir, config.exp_name), False)\n",
    "\n",
    "print(\"Using MFG Dataset\")\n",
    "dataset = MFG_Dataset(\n",
    "    dataset = full_dataset,\n",
    "    config = config,\n",
    "    eval_mode = False\n",
    ")\n",
    "eval_dataset = MFG_Dataset(\n",
    "    dataset = full_dataset,\n",
    "    config = config,\n",
    "    eval_mode = True\n",
    ")\n",
    "\n",
    "audioset_data = data_prep(dataset, eval_dataset, device_num)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor = \"acc\",\n",
    "    filename='l-{epoch:d}-{acc:.3f}',\n",
    "    save_top_k = 20,\n",
    "    mode = \"max\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:26:46.641482400Z",
     "start_time": "2025-03-27T14:26:46.607218600Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Set the Trainer\u001B[39;00m\n\u001B[0;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(\n\u001B[0;32m      3\u001B[0m     deterministic\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m----> 4\u001B[0m     default_root_dir\u001B[38;5;241m=\u001B[39m\u001B[43mcheckpoint_dir\u001B[49m,\n\u001B[0;32m      5\u001B[0m     gpus\u001B[38;5;241m=\u001B[39mdevice_num, \n\u001B[0;32m      6\u001B[0m     val_check_interval\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m,\n\u001B[0;32m      7\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmax_epoch,\n\u001B[0;32m      8\u001B[0m     auto_lr_find\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,    \n\u001B[0;32m      9\u001B[0m     sync_batchnorm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     10\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[checkpoint_callback],\n\u001B[0;32m     11\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mddp\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m device_num \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     12\u001B[0m     num_sanity_val_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m     13\u001B[0m     resume_from_checkpoint\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \n\u001B[0;32m     14\u001B[0m     replace_sampler_ddp\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     15\u001B[0m     gradient_clip_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m\n\u001B[0;32m     16\u001B[0m )\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Create the HTSAT model with updated channel input (e.g., 3 or 6 channels)\u001B[39;00m\n\u001B[0;32m     19\u001B[0m sed_model \u001B[38;5;241m=\u001B[39m HTSAT_Swin_Transformer(\n\u001B[0;32m     20\u001B[0m     spec_size\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mhtsat_spec_size,\n\u001B[0;32m     21\u001B[0m     patch_size\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mhtsat_patch_size,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     29\u001B[0m     num_heads\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mhtsat_num_head\n\u001B[0;32m     30\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'checkpoint_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the Trainer\n",
    "trainer = pl.Trainer(\n",
    "    deterministic=False,\n",
    "    default_root_dir=checkpoint_dir,\n",
    "    gpus=device_num, \n",
    "    val_check_interval=1.0,\n",
    "    max_epochs=config.max_epoch,\n",
    "    auto_lr_find=True,    \n",
    "    sync_batchnorm=True,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accelerator=\"ddp\" if device_num > 1 else None,\n",
    "    num_sanity_val_steps=0,\n",
    "    resume_from_checkpoint=None, \n",
    "    replace_sampler_ddp=False,\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "# Create the HTSAT model with updated channel input (e.g., 3 or 6 channels)\n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "    spec_size=config.htsat_spec_size,\n",
    "    patch_size=config.htsat_patch_size,\n",
    "    in_chans=3,  # Change to 3 or 6 depending on your sensor data channels\n",
    "    num_classes=config.classes_num,\n",
    "    window_size=config.htsat_window_size,\n",
    "    config=config,\n",
    "    depths=config.htsat_depth,\n",
    "    embed_dim=config.htsat_dim,\n",
    "    patch_stride=config.htsat_stride,\n",
    "    num_heads=config.htsat_num_head\n",
    ")\n",
    "\n",
    "model = SEDWrapper(\n",
    "    sed_model=sed_model, \n",
    "    config=config,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "if config.resume_checkpoint is not None:\n",
    "    print(\"Load Checkpoint from \", config.resume_checkpoint)\n",
    "    ckpt = torch.load(config.resume_checkpoint, map_location=\"cpu\")\n",
    "    \n",
    "    key = \"sed_model.patch_embed.proj.weight\"\n",
    "    if key in ckpt[\"state_dict\"]:\n",
    "        weight = ckpt[\"state_dict\"][key]\n",
    "        # Adapt the patch embedding weights to match the current in_chans setting\n",
    "        if weight.shape[1] != sed_model.in_chans:\n",
    "            # Assume the checkpoint was trained with a single channel (in_chans==1)\n",
    "            if weight.shape[1] == 1:\n",
    "                adapted_weight = weight.repeat(1, sed_model.in_chans, 1, 1) / sed_model.in_chans\n",
    "                ckpt[\"state_dict\"][key] = adapted_weight\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected number of channels in checkpoint weight: {}\".format(weight.shape[1]))\n",
    "    \n",
    "    # Remove keys that might conflict with the current model architecture\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.head.weight\", None)\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.head.bias\", None)\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.tscam_conv.weight\", None)\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.tscam_conv.bias\", None)\n",
    "    \n",
    "    model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\Louis\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\core\\datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                   | Params\n",
      "-----------------------------------------------------\n",
      "0 | sed_model | HTSAT_Swin_Transformer | 31.3 M\n",
      "-----------------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.301   Total estimated model params size (MB)\n",
      "C:\\Users\\Louis\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Louis\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:432: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Louis\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5faab5272b2d4090b8789d64b8739cc7"
      },
      "application/json": {
       "n": 0,
       "total": null,
       "elapsed": 0.005504608154296875,
       "ncols": null,
       "nrows": null,
       "prefix": "Training",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [6, 6505467] at entry 0 and [6, 9045600] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 75\u001B[0m\n\u001B[0;32m     65\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(\n\u001B[0;32m     66\u001B[0m     default_root_dir\u001B[38;5;241m=\u001B[39mcheckpoint_dir,\n\u001B[0;32m     67\u001B[0m     gpus\u001B[38;5;241m=\u001B[39mdevice_num,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     71\u001B[0m     num_sanity_val_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m     72\u001B[0m )\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting training…\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 75\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maudioset_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:740\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001B[0m\n\u001B[0;32m    735\u001B[0m     rank_zero_deprecation(\n\u001B[0;32m    736\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    737\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    738\u001B[0m     )\n\u001B[0;32m    739\u001B[0m     train_dataloaders \u001B[38;5;241m=\u001B[39m train_dataloader\n\u001B[1;32m--> 740\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    741\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    742\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:685\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[1;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    676\u001B[0m \u001B[38;5;124;03mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001B[39;00m\n\u001B[0;32m    677\u001B[0m \u001B[38;5;124;03mas all errors should funnel through them\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    682\u001B[0m \u001B[38;5;124;03m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001B[39;00m\n\u001B[0;32m    683\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    684\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 685\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    686\u001B[0m \u001B[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[39;00m\n\u001B[0;32m    687\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:777\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    775\u001B[0m \u001B[38;5;66;03m# TODO: ckpt_path only in v1.7\u001B[39;00m\n\u001B[0;32m    776\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[1;32m--> 777\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    779\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1199\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m   1196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[0;32m   1198\u001B[0m \u001B[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001B[39;00m\n\u001B[1;32m-> 1199\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001B[39;00m\n\u001B[0;32m   1202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post_dispatch()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1279\u001B[0m, in \u001B[0;36mTrainer._dispatch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1277\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_type_plugin\u001B[38;5;241m.\u001B[39mstart_predicting(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m   1278\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1279\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_type_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_training\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py:202\u001B[0m, in \u001B[0;36mTrainingTypePlugin.start_training\u001B[1;34m(self, trainer)\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstart_training\u001B[39m(\u001B[38;5;28mself\u001B[39m, trainer: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.Trainer\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;66;03m# double dispatch to initiate the training loop\u001B[39;00m\n\u001B[1;32m--> 202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_results \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1289\u001B[0m, in \u001B[0;36mTrainer.run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1287\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[0;32m   1288\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[1;32m-> 1289\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1319\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mtrainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[0;32m   1318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m-> 1319\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:145\u001B[0m, in \u001B[0;36mLoop.run\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 145\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[0;32m    147\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrestarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:234\u001B[0m, in \u001B[0;36mFitLoop.advance\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    231\u001B[0m data_fetcher \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_data_connector\u001B[38;5;241m.\u001B[39mget_profiled_dataloader(dataloader)\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 234\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    236\u001B[0m     \u001B[38;5;66;03m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001B[39;00m\n\u001B[0;32m    237\u001B[0m     \u001B[38;5;66;03m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001B[39;00m\n\u001B[0;32m    238\u001B[0m     \u001B[38;5;66;03m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001B[39;00m\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;66;03m# TODO(@carmocca): deprecate and rename so users don't get confused\u001B[39;00m\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:140\u001B[0m, in \u001B[0;36mLoop.run\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_skip()\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset()\n\u001B[1;32m--> 140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_run_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:141\u001B[0m, in \u001B[0;36mTrainingEpochLoop.on_run_start\u001B[1;34m(self, data_fetcher, **kwargs)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mepoch_progress\u001B[38;5;241m.\u001B[39mincrement_started()\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reload_dataloader_state_dict(data_fetcher)\n\u001B[1;32m--> 141\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[43m_update_dataloader_iter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_idx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:121\u001B[0m, in \u001B[0;36m_update_dataloader_iter\u001B[1;34m(data_fetcher, batch_idx)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Attach the dataloader.\"\"\"\u001B[39;00m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# restore iteration\u001B[39;00m\n\u001B[1;32m--> 121\u001B[0m     dataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    123\u001B[0m     dataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(data_fetcher)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:199\u001B[0m, in \u001B[0;36mAbstractDataFetcher.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_patch()\n\u001B[1;32m--> 199\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprefetching\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprefetch_batches\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:258\u001B[0m, in \u001B[0;36mDataFetcher.prefetching\u001B[1;34m(self, prefetch_batches)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(prefetch_batches):\n\u001B[0;32m    257\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 258\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fetch_next_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:300\u001B[0m, in \u001B[0;36mDataFetcher._fetch_next_batch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    298\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_fetch_start()\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_profiler(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfetch_next_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 300\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfetched \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_fetch_end(batch, data)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:550\u001B[0m, in \u001B[0;36mCombinedLoaderIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    544\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    545\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Fetches the next batch from multiple data loaders.\u001B[39;00m\n\u001B[0;32m    546\u001B[0m \n\u001B[0;32m    547\u001B[0m \u001B[38;5;124;03m    Returns:\u001B[39;00m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;124;03m        a collections of batch data\u001B[39;00m\n\u001B[0;32m    549\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 550\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest_next_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader_iters\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:562\u001B[0m, in \u001B[0;36mCombinedLoaderIterator.request_next_batch\u001B[1;34m(loader_iters)\u001B[0m\n\u001B[0;32m    552\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    553\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrequest_next_batch\u001B[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    554\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the batch of data from multiple iterators.\u001B[39;00m\n\u001B[0;32m    555\u001B[0m \n\u001B[0;32m    556\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    560\u001B[0m \u001B[38;5;124;03m        Any: a collections of batch data\u001B[39;00m\n\u001B[0;32m    561\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mapply_to_collection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloader_iters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\utilities\\apply_func.py:95\u001B[0m, in \u001B[0;36mapply_to_collection\u001B[1;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;66;03m# Breaking condition\u001B[39;00m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, dtype) \u001B[38;5;129;01mand\u001B[39;00m (wrong_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, wrong_dtype)):\n\u001B[1;32m---> 95\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m function(data, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     97\u001B[0m elem_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(data)\n\u001B[0;32m     99\u001B[0m \u001B[38;5;66;03m# Recursively apply to collection items\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\utilities\\auto_restart.py:474\u001B[0m, in \u001B[0;36m_capture_metadata_collate\u001B[1;34m(samples, dataset, default_collate)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_capture_metadata_collate\u001B[39m(samples: List, dataset: Dataset, default_collate: Callable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict:\n\u001B[0;32m    463\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"A collate function that adds the state dict of a :class:`CaptureIterableDataset` or\u001B[39;00m\n\u001B[0;32m    464\u001B[0m \u001B[38;5;124;03m    :class:`CaptureMapDataset` used in the worker processes. This function gets executed within the worker\u001B[39;00m\n\u001B[0;32m    465\u001B[0m \u001B[38;5;124;03m    processes. The structure will be:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    472\u001B[0m \u001B[38;5;124;03m        }\u001B[39;00m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 474\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mdefault_collate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataset, (CaptureIterableDataset, CaptureMapDataset)):\n\u001B[0;32m    476\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "Cell \u001B[1;32mIn[10], line 21\u001B[0m, in \u001B[0;36mpad_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m     18\u001B[0m     names\u001B[38;5;241m.\u001B[39mappend(sample[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maudio_name\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     19\u001B[0m     lengths\u001B[38;5;241m.\u001B[39mappend(sample[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreal_len\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m---> 21\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwaveform\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaves\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(targets),\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maudio_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: names,\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreal_len\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(lengths),\n\u001B[0;32m     25\u001B[0m }\n",
      "\u001B[1;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [6, 6505467] at entry 0 and [6, 9045600] at entry 1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "def dynamic_pad_collate(batch):\n",
    "    # Convert each sample's waveform to a tensor (if needed)\n",
    "    for sample in batch:\n",
    "        if not isinstance(sample[\"waveform\"], torch.Tensor):\n",
    "            sample[\"waveform\"] = torch.tensor(sample[\"waveform\"], dtype=torch.float)\n",
    "    # Determine the maximum time dimension (axis=1) among samples\n",
    "    max_length = max(sample[\"waveform\"].shape[1] for sample in batch)\n",
    "    padded_waveforms, targets, names, lengths = [], [], [], []\n",
    "    for sample in batch:\n",
    "        wav = sample[\"waveform\"]  # shape: [3, time]\n",
    "        current_length = wav.shape[1]\n",
    "        if current_length < max_length:\n",
    "            pad_amount = int(max_length - current_length)\n",
    "            wav = F.pad(wav, (0, pad_amount), mode=\"constant\", value=0)\n",
    "        padded_waveforms.append(wav)\n",
    "        targets.append(sample[\"target\"])\n",
    "        names.append(sample[\"audio_name\"])\n",
    "        lengths.append(current_length)\n",
    "    return {\n",
    "        \"waveform\": torch.stack(padded_waveforms),  # shape: [B, 3, max_length]\n",
    "        \"target\": torch.tensor(targets),\n",
    "        \"audio_name\": names,\n",
    "        \"real_len\": torch.tensor(lengths)\n",
    "    }\n",
    "\n",
    "# Instantiate DataModule and ensure setup()\n",
    "audioset_data = data_prep(full_dataset, config, device_num)\n",
    "audioset_data.setup(\"fit\")\n",
    "if hasattr(audioset_data, \"trainer\"):\n",
    "    del audioset_data.trainer\n",
    "\n",
    "# Override DataLoaders to use pad_collate\n",
    "audioset_data.train_dataloader = lambda: DataLoader(\n",
    "    audioset_data.train_dataset,\n",
    "    batch_size=config.batch_size // max(1, device_num),\n",
    "    sampler=DistributedSampler(audioset_data.train_dataset) if device_num > 1 else None,\n",
    "    num_workers=0,\n",
    "    collate_fn=pad_collate,\n",
    ")\n",
    "audioset_data.val_dataloader = lambda: DataLoader(\n",
    "    audioset_data.eval_dataset,\n",
    "    batch_size=config.batch_size // max(1, device_num),\n",
    "    sampler=DistributedSampler(audioset_data.eval_dataset) if device_num > 1 else None,\n",
    "    num_workers=0,\n",
    "    collate_fn=pad_collate,\n",
    ")\n",
    "\n",
    "# Build model with mono input\n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "    spec_size=config.htsat_spec_size,\n",
    "    patch_size=config.htsat_patch_size,\n",
    "    in_chans=3,  # use 3-channel input\n",
    "    num_classes=config.classes_num,\n",
    "    window_size=config.htsat_window_size,\n",
    "    config=config,\n",
    "    depths=config.htsat_depth,\n",
    "    embed_dim=config.htsat_dim,\n",
    "    patch_stride=config.htsat_stride,\n",
    "    num_heads=config.htsat_num_head,\n",
    ")\n",
    "model = SEDWrapper(sed_model=sed_model, config=config, dataset=audioset_data.train_dataset)\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=checkpoint_dir,\n",
    "    gpus=device_num,\n",
    "    max_epochs=config.max_epoch,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accelerator=\"ddp\" if device_num > 1 else None,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "print(\"Starting training…\")\n",
    "trainer.fit(model, audioset_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-25T14:27:47.973102200Z",
     "start_time": "2025-03-25T14:27:43.598530500Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let us Check the Result\n",
    "\n",
    "Find the path of your saved checkpoint and paste it in the below variable.\n",
    "Then you are able to follow the below code for checking the prediction result of any sample you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T13:02:39.930853500Z",
     "start_time": "2025-03-25T13:02:39.907467300Z"
    }
   },
   "outputs": [],
   "source": [
    "# infer the single data to check the result\n",
    "# get a model you saved\n",
    "model_path = r\"C:\\Users\\Louis\\PycharmProjects\\HTS-AT(Conda)\\HTS-Audio-Transformer\\workspace\\results\\exp_htsat_esc_50\\checkpoint\\lightning_logs\\version_1\\checkpoints\\l-epoch=4-acc=0.815.ckpt\"\n",
    "\n",
    "# get the groundtruth\n",
    "meta = np.loadtxt(meta_path , delimiter=',', dtype='str', skiprows=1)\n",
    "gd = {}\n",
    "for label in meta:\n",
    "    name = label[0]\n",
    "    target = label[2]\n",
    "    gd[name] = target\n",
    "\n",
    "class Audio_Classification:\n",
    "    def __init__(self, model_path, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device('cuda')\n",
    "        self.sed_model = HTSAT_Swin_Transformer(\n",
    "            spec_size=config.htsat_spec_size,\n",
    "            patch_size=config.htsat_patch_size,\n",
    "            in_chans=1,\n",
    "            num_classes=config.classes_num,\n",
    "            window_size=config.htsat_window_size,\n",
    "            config = config,\n",
    "            depths = config.htsat_depth,\n",
    "            embed_dim = config.htsat_dim,\n",
    "            patch_stride=config.htsat_stride,\n",
    "            num_heads=config.htsat_num_head\n",
    "        )\n",
    "        ckpt = torch.load(model_path, map_location=\"cpu\")\n",
    "        temp_ckpt = {}\n",
    "        for key in ckpt[\"state_dict\"]:\n",
    "            temp_ckpt[key[10:]] = ckpt['state_dict'][key]\n",
    "        self.sed_model.load_state_dict(temp_ckpt)\n",
    "        self.sed_model.to(self.device)\n",
    "        self.sed_model.eval()\n",
    "\n",
    "\n",
    "    def predict(self, audiofile):\n",
    "\n",
    "        if audiofile:\n",
    "            waveform, sr = librosa.load(audiofile, sr=32000)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = torch.from_numpy(waveform).float().to(self.device)\n",
    "                output_dict = self.sed_model(x[None, :], None, True)\n",
    "                pred = output_dict['clipwise_output']\n",
    "                pred_post = pred[0].detach().cpu().numpy()\n",
    "                pred_label = np.argmax(pred_post)\n",
    "                pred_prob = np.max(pred_post)\n",
    "            return pred_label, pred_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch = next(iter(audioset_data.train_dataloader()))\n",
    "x = batch[\"waveform\"].to(device)  # shape should be (batch_size, 3, clip_samples)\n",
    "out = sed_model(x)                # no channel‐mismatch error\n",
    "print(\"Output keys:\", out.keys())\n",
    "print(\"Clipwise output shape:\", out[\"clipwise_output\"].shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:43:04.286031600Z",
     "start_time": "2025-03-13T13:42:59.894874200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_11376\\4270949661.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audiocls predict output:  13 8.718129 13\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "Audiocls = Audio_Classification(model_path, config)\n",
    "\n",
    "# pick any audio you like in the ESC-50 testing set (cross-validation)\n",
    "pred_label, pred_prob = Audiocls.predict(\"./workspace/esc-50/raw/ESC-50-master/audio/1-7456-A-13.wav\")\n",
    "\n",
    "print('Audiocls predict output: ', pred_label, pred_prob, gd[\"1-7456-A-13.wav\"])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "htsat_env",
   "language": "python",
   "display_name": "Python (HTSAT_env)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb1a0df39641c41734bdd2d42699ec57167c4cf18fd061cdef52c16cce6262af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
