{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial on training a HTS-AT model for audio classification on the ESC-50 Dataset\n",
    "\n",
    "Referece: \n",
    "\n",
    "[HTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection, ICASSP 2022](https://arxiv.org/abs/2202.00874)\n",
    "\n",
    "Following the HTS-AT's paper, in this tutorial, we would show how to use the HST-AT in the training of the ESC-50 Dataset.\n",
    "\n",
    "The [ESC-50 dataset](https://github.com/karolpiczak/ESC-50) is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories\n",
    "\n",
    "Before running this tutorial, please make sure that you install the below packages by following steps:\n",
    "\n",
    "1. download [the codebase](https://github.com/RetroCirce/HTS-Audio-Transformer), and put this tutorial notebook inside the codebase folder.\n",
    "\n",
    "2. In the github code folder:\n",
    "\n",
    "    > pip install -r requirements.txt\n",
    "\n",
    "3. We do not include the installation of PyTorch in the requirment, since different machines require different vereions of CUDA and Toolkits. So make sure you install the PyTorch from [the official guidance](https://pytorch.org/).\n",
    "\n",
    "4. Install the 'SOX' and the 'ffmpeg', we recommend that you run this code in Linux inside the Conda environment. In that, you can install them by:\n",
    "\n",
    "    > sudo apt install sox\n",
    "    \n",
    "    > conda install -c conda-forge ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:35:28.498060400Z",
     "start_time": "2025-03-27T14:35:26.335573700Z"
    }
   },
   "outputs": [],
   "source": [
    "# import basic packages\n",
    "import os\n",
    "import numpy as np\n",
    "import wget\n",
    "import sys\n",
    "import gdown\n",
    "import zipfile\n",
    "import librosa\n",
    "# in the notebook, we only can use one GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:35:28.519530900Z",
     "start_time": "2025-03-27T14:35:28.504148300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the workspace and download the needed files\n",
    "\n",
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "workspace = \"./workspace_ADS_v2\"\n",
    "dataset_path = os.path.join(workspace, \"mfg_robot\")\n",
    "checkpoint_path = os.path.join(workspace, \"ckpt\")\n",
    "mfg_raw_path = os.path.join(dataset_path, 'raw')\n",
    "\n",
    "\n",
    "create_path(workspace)\n",
    "create_path(dataset_path)\n",
    "create_path(checkpoint_path)\n",
    "create_path(mfg_raw_path)\n",
    "\n",
    "\n",
    "# # download the esc-50 dataset\n",
    "# \n",
    "# if not os.path.exists(os.path.join(dataset_path, 'ESC-50-master.zip')):\n",
    "#     print(\"-------------Downloading ESC-50 Dataset-------------\")\n",
    "#     wget.download('https://github.com/karoldvl/ESC-50/archive/master.zip', out=dataset_path)\n",
    "#     with zipfile.ZipFile(os.path.join(dataset_path, 'ESC-50-master.zip'), 'r') as zip_ref:\n",
    "#         zip_ref.extractall(esc_raw_path)\n",
    "#     print(\"-------------Success-------------\")\n",
    "# \n",
    "# if not os.path.exists(os.path.join(checkpoint_path,'htsat_audioset_pretrain.ckpt')):\n",
    "#     gdown.download(id='1OK8a5XuMVLyeVKF117L8pfxeZYdfSDZv', output=os.path.join(checkpoint_path,'htsat_audioset_pretrain.ckpt'))\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Resample ESC-50-------------\n",
      "-------------Resample Success-------------\n"
     ]
    }
   ],
   "source": [
    "# Process Manufacturing Dataset – Resampling Audio Files\n",
    "\n",
    "audio_path = os.path.join(mfg_raw_path, 'MFG-master', 'audio')\n",
    "resample_path = os.path.join(dataset_path, 'resample')\n",
    "savedata_path = os.path.join(dataset_path, 'mfg-data.npy')\n",
    "create_path(resample_path)\n",
    "\n",
    "audio_list = os.listdir(audio_path)\n",
    "\n",
    "print(\"-------------Resample ESC-50-------------\")\n",
    "for f in audio_list:\n",
    "    full_f = os.path.join(audio_path, f)\n",
    "    resample_f = os.path.join(resample_path, f)\n",
    "    if not os.path.exists(resample_f):\n",
    "        os.system('sox -V1 ' + full_f + ' -r 32000 ' + resample_f)\n",
    "print(\"-------------Resample Success-------------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T14:35:28.557364300Z",
     "start_time": "2025-03-27T14:35:28.516526500Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Build Dataset-------------\n",
      "-------------Build Dataset Success-------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Paths\n",
    "meta_path = os.path.join(mfg_raw_path, 'MFG-master\\meta\\mfg.csv')  # Adjust this path if needed\n",
    "meta = np.loadtxt(meta_path, delimiter=',', dtype='str', skiprows=1)\n",
    "\n",
    "print(\"-------------Build Dataset-------------\")\n",
    "output_dict = [[] for _ in range(5)]  # Assuming 5 folds, still okay if we only use fold 1\n",
    "\n",
    "for label in meta:\n",
    "    name = label[0]\n",
    "    fold = int(label[1])\n",
    "    target = int(label[2])\n",
    "    \n",
    "    #y, sr = librosa.load(os.path.join(resample_path, name), sr=None)\n",
    "    \n",
    "    # Preserve the orinal multi-channel structure for the sensor data\n",
    "    y, sr = librosa.load(os.path.join(resample_path, name), sr=None, mono=False)\n",
    "\n",
    "    output_dict[fold - 1].append({\n",
    "        \"name\": name,\n",
    "        \"target\": target,\n",
    "        \"waveform\": y\n",
    "    })\n",
    "\n",
    "np.save(savedata_path, np.array(output_dict, dtype=object))\n",
    "\n",
    "print(\"-------------Build Dataset Success-------------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T14:35:28.806324400Z",
     "start_time": "2025-03-27T14:35:28.535796800Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:35:36.316462900Z",
     "start_time": "2025-03-27T14:35:28.811323400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model package\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "\n",
    "from utils import create_folder, dump_config, process_idc\n",
    "import mfg_config as config\n",
    "from sed_model import SEDWrapper, Ensemble_SEDWrapper\n",
    "from data_generator_mfg import MFG_Dataset\n",
    "from model.htsat_mfg_v_2_0 import HTSAT_Swin_Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "# New data preparation class\n",
    "class data_prep(pl.LightningDataModule):\n",
    "    def __init__(self, dataset, config, device_num):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset  # Store only a reference\n",
    "        self.config = config\n",
    "        self.device_num = device_num\n",
    "        self.train_dataset = None  # Placeholder, will be initialized later\n",
    "        self.eval_dataset = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"This method is called inside Lightning, and it ensures datasets are created properly.\"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = MFG_Dataset(\n",
    "                dataset=self.dataset,\n",
    "                config=self.config,\n",
    "                eval_mode=False\n",
    "            )\n",
    "            self.eval_dataset = MFG_Dataset(\n",
    "                dataset=self.dataset,\n",
    "                config=self.config,\n",
    "                eval_mode=True\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_sampler = DistributedSampler(self.train_dataset, shuffle=False) if self.device_num > 1 else None\n",
    "        return DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            num_workers=self.config.num_workers,\n",
    "            batch_size=self.config.batch_size // max(1, self.device_num),\n",
    "            shuffle=False,\n",
    "            sampler=train_sampler\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        eval_sampler = DistributedSampler(self.eval_dataset, shuffle=False) if self.device_num > 1 else None\n",
    "        return DataLoader(\n",
    "            dataset=self.eval_dataset,\n",
    "            num_workers=self.config.num_workers,\n",
    "            batch_size=self.config.batch_size // max(1, self.device_num),\n",
    "            shuffle=False,\n",
    "            sampler=eval_sampler\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_sampler = DistributedSampler(self.eval_dataset, shuffle=False) if self.device_num > 1 else None\n",
    "        return DataLoader(\n",
    "            dataset=self.eval_dataset,\n",
    "            num_workers=self.config.num_workers,\n",
    "            batch_size=self.config.batch_size // max(1, self.device_num),\n",
    "            shuffle=False,\n",
    "            sampler=test_sampler\n",
    "        )\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        \"\"\"Removes unpicklable attributes before multiprocessing starts\"\"\"\n",
    "        for attr in [\"trainer\", \"prepare_data\", \"setup\", \"teardown\"]:\n",
    "            if hasattr(self, attr):\n",
    "                delattr(self, attr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T14:35:36.349978200Z",
     "start_time": "2025-03-27T14:35:36.331681500Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:35:36.462808500Z",
     "start_time": "2025-03-27T14:35:36.351988200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_num: 1\n",
      "each batch size: 64\n",
      "Using MFG Dataset\n"
     ]
    }
   ],
   "source": [
    "# Set the workspace\n",
    "device_num = torch.cuda.device_count()\n",
    "print(\"device_num:\", device_num)\n",
    "print(\"each batch size:\", config.batch_size // device_num)\n",
    "\n",
    "full_dataset = np.load(os.path.join(config.dataset_path, \"mfg-data.npy\"), allow_pickle = True)\n",
    "\n",
    "# set exp folder\n",
    "exp_dir = os.path.join(config.workspace, \"results\", config.exp_name)\n",
    "checkpoint_dir = os.path.join(config.workspace, \"results\", config.exp_name, \"checkpoint\")\n",
    "if not config.debug:\n",
    "    create_folder(os.path.join(config.workspace, \"results\"))\n",
    "    create_folder(exp_dir)\n",
    "    create_folder(checkpoint_dir)\n",
    "    dump_config(config, os.path.join(exp_dir, config.exp_name), False)\n",
    "\n",
    "print(\"Using MFG Dataset\")\n",
    "dataset = MFG_Dataset(\n",
    "    dataset = full_dataset,\n",
    "    config = config,\n",
    "    eval_mode = False\n",
    ")\n",
    "eval_dataset = MFG_Dataset(\n",
    "    dataset = full_dataset,\n",
    "    config = config,\n",
    "    eval_mode = True\n",
    ")\n",
    "\n",
    "audioset_data = data_prep(dataset, eval_dataset, device_num)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor = \"acc\",\n",
    "    filename='l-{epoch:d}-{acc:.3f}',\n",
    "    save_top_k = 20,\n",
    "    mode = \"max\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:35:50.581034500Z",
     "start_time": "2025-03-27T14:35:48.517877300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\Louis\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Checkpoint from  ./workspace_ADS_v2/ckpt/htsat_audioset_pretrain.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_33472\\308525678.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(config.resume_checkpoint, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Set the Trainer\n",
    "trainer = pl.Trainer(\n",
    "    deterministic=False,\n",
    "    default_root_dir=checkpoint_dir,\n",
    "    gpus=device_num, \n",
    "    val_check_interval=1.0,\n",
    "    max_epochs=config.max_epoch,\n",
    "    auto_lr_find=True,    \n",
    "    sync_batchnorm=True,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accelerator=\"ddp\" if device_num > 1 else None,\n",
    "    num_sanity_val_steps=0,\n",
    "    resume_from_checkpoint=None, \n",
    "    replace_sampler_ddp=False,\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "# Create the HTSAT model with updated channel input (e.g., 3 or 6 channels)\n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "    spec_size=config.htsat_spec_size,\n",
    "    patch_size=config.htsat_patch_size,\n",
    "    in_chans=3,  # Change to 3 or 6 depending on your sensor data channels\n",
    "    num_classes=config.classes_num,\n",
    "    window_size=config.htsat_window_size,\n",
    "    config=config,\n",
    "    depths=config.htsat_depth,\n",
    "    embed_dim=config.htsat_dim,\n",
    "    patch_stride=config.htsat_stride,\n",
    "    num_heads=config.htsat_num_head\n",
    ")\n",
    "\n",
    "model = SEDWrapper(\n",
    "    sed_model=sed_model, \n",
    "    config=config,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "if config.resume_checkpoint is not None:\n",
    "    print(\"Load Checkpoint from \", config.resume_checkpoint)\n",
    "    ckpt = torch.load(config.resume_checkpoint, map_location=\"cpu\")\n",
    "    \n",
    "    key = \"sed_model.patch_embed.proj.weight\"\n",
    "    if key in ckpt[\"state_dict\"]:\n",
    "        weight = ckpt[\"state_dict\"][key]\n",
    "        # Adapt the patch embedding weights to match the current in_chans setting\n",
    "        if weight.shape[1] != sed_model.in_chans:\n",
    "            # Assume the checkpoint was trained with a single channel (in_chans==1)\n",
    "            if weight.shape[1] == 1:\n",
    "                adapted_weight = weight.repeat(1, sed_model.in_chans, 1, 1) / sed_model.in_chans\n",
    "                ckpt[\"state_dict\"][key] = adapted_weight\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected number of channels in checkpoint weight: {}\".format(weight.shape[1]))\n",
    "    \n",
    "    # Remove keys that might conflict with the current model architecture\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.head.weight\", None)\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.head.bias\", None)\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.tscam_conv.weight\", None)\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.tscam_conv.bias\", None)\n",
    "    \n",
    "    model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                   | Params\n",
      "-----------------------------------------------------\n",
      "0 | sed_model | HTSAT_Swin_Transformer | 31.3 M\n",
      "-----------------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.301   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training…\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "927064b8ee244b5482b2b4ba9d54c06e"
      },
      "application/json": {
       "n": 0,
       "total": null,
       "elapsed": 0.0064351558685302734,
       "ncols": null,
       "nrows": null,
       "prefix": "Training",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24a8977edae14d0c8b1ee449a8ff4823"
      },
      "application/json": {
       "n": 0,
       "total": null,
       "elapsed": 0.0051348209381103516,
       "ncols": null,
       "nrows": null,
       "prefix": "Validating",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 {'acc': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validating: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8784bbc782b9446ba9393a88ed6b24ff"
      },
      "application/json": {
       "n": 0,
       "total": null,
       "elapsed": 0.005991935729980469,
       "ncols": null,
       "nrows": null,
       "prefix": "Validating",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 {'acc': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "def dynamic_pad_collate(batch):\n",
    "    # Convert each sample's waveform to a tensor (if needed)\n",
    "    for sample in batch:\n",
    "        if not isinstance(sample[\"waveform\"], torch.Tensor):\n",
    "            sample[\"waveform\"] = torch.tensor(sample[\"waveform\"], dtype=torch.float)\n",
    "        # Ensure the waveform has at least two dimensions: [channels, time]\n",
    "        if sample[\"waveform\"].dim() == 1:\n",
    "            sample[\"waveform\"] = sample[\"waveform\"].unsqueeze(0)\n",
    "    # Determine the maximum time dimension (axis=1) among samples\n",
    "    max_length = max(sample[\"waveform\"].shape[1] for sample in batch)\n",
    "    padded_waveforms, targets, names, lengths = [], [], [], []\n",
    "    for sample in batch:\n",
    "        wav = sample[\"waveform\"]  # expected shape: [channels, time]\n",
    "        current_length = wav.shape[1]\n",
    "        if current_length < max_length:\n",
    "            pad_amount = int(max_length - current_length)\n",
    "            # Pad along the time dimension\n",
    "            wav = F.pad(wav, (0, pad_amount), mode=\"constant\", value=0)\n",
    "        padded_waveforms.append(wav)\n",
    "        targets.append(sample[\"target\"])\n",
    "        names.append(sample[\"audio_name\"])\n",
    "        lengths.append(current_length)\n",
    "    return {\n",
    "        \"waveform\": torch.stack(padded_waveforms),  # shape: [B, channels, max_length]\n",
    "        \"target\": torch.tensor(targets),\n",
    "        \"audio_name\": names,\n",
    "        \"real_len\": torch.tensor(lengths)\n",
    "    }\n",
    "\n",
    "# Instantiate DataModule and ensure setup()\n",
    "audioset_data = data_prep(full_dataset, config, device_num)\n",
    "audioset_data.setup(\"fit\")\n",
    "if hasattr(audioset_data, \"trainer\"):\n",
    "    del audioset_data.trainer\n",
    "\n",
    "# Override DataLoaders to use dynamic_pad_collate\n",
    "audioset_data.train_dataloader = lambda: DataLoader(\n",
    "    audioset_data.train_dataset,\n",
    "    batch_size=config.batch_size // max(1, device_num),\n",
    "    sampler=DistributedSampler(audioset_data.train_dataset) if device_num > 1 else None,\n",
    "    num_workers=0,  # For debugging; increase as needed later\n",
    "    collate_fn=dynamic_pad_collate\n",
    ")\n",
    "audioset_data.val_dataloader = lambda: DataLoader(\n",
    "    audioset_data.eval_dataset,\n",
    "    batch_size=config.batch_size // max(1, device_num),\n",
    "    sampler=DistributedSampler(audioset_data.eval_dataset) if device_num > 1 else None,\n",
    "    num_workers=0,\n",
    "    collate_fn=dynamic_pad_collate\n",
    ")\n",
    "\n",
    "# Build model with 3-channel input (since your data now has 3 channels)\n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "    spec_size=config.htsat_spec_size,\n",
    "    patch_size=config.htsat_patch_size,\n",
    "    in_chans=1,  # Use 3-channel input\n",
    "    num_classes=config.classes_num,\n",
    "    window_size=config.htsat_window_size,\n",
    "    config=config,\n",
    "    depths=config.htsat_depth,\n",
    "    embed_dim=config.htsat_dim,\n",
    "    patch_stride=config.htsat_stride,\n",
    "    num_heads=config.htsat_num_head\n",
    ")\n",
    "model = SEDWrapper(sed_model=sed_model, config=config, dataset=audioset_data.train_dataset)\n",
    "\n",
    "# Trainer setup\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=checkpoint_dir,\n",
    "    gpus=device_num,\n",
    "    max_epochs=config.max_epoch,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accelerator=\"ddp\" if device_num > 1 else None,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "print(\"Starting training…\")\n",
    "trainer.fit(model, audioset_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T14:52:19.031611100Z",
     "start_time": "2025-03-27T14:52:10.435055200Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let us Check the Result\n",
    "\n",
    "Find the path of your saved checkpoint and paste it in the below variable.\n",
    "Then you are able to follow the below code for checking the prediction result of any sample you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T13:02:39.930853500Z",
     "start_time": "2025-03-25T13:02:39.907467300Z"
    }
   },
   "outputs": [],
   "source": [
    "# infer the single data to check the result\n",
    "# get a model you saved\n",
    "model_path = r\"C:\\Users\\Louis\\PycharmProjects\\HTS-AT(Conda)\\HTS-Audio-Transformer\\workspace\\results\\exp_htsat_esc_50\\checkpoint\\lightning_logs\\version_1\\checkpoints\\l-epoch=4-acc=0.815.ckpt\"\n",
    "\n",
    "# get the groundtruth\n",
    "meta = np.loadtxt(meta_path , delimiter=',', dtype='str', skiprows=1)\n",
    "gd = {}\n",
    "for label in meta:\n",
    "    name = label[0]\n",
    "    target = label[2]\n",
    "    gd[name] = target\n",
    "\n",
    "class Audio_Classification:\n",
    "    def __init__(self, model_path, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device('cuda')\n",
    "        self.sed_model = HTSAT_Swin_Transformer(\n",
    "            spec_size=config.htsat_spec_size,\n",
    "            patch_size=config.htsat_patch_size,\n",
    "            in_chans=1,\n",
    "            num_classes=config.classes_num,\n",
    "            window_size=config.htsat_window_size,\n",
    "            config = config,\n",
    "            depths = config.htsat_depth,\n",
    "            embed_dim = config.htsat_dim,\n",
    "            patch_stride=config.htsat_stride,\n",
    "            num_heads=config.htsat_num_head\n",
    "        )\n",
    "        ckpt = torch.load(model_path, map_location=\"cpu\")\n",
    "        temp_ckpt = {}\n",
    "        for key in ckpt[\"state_dict\"]:\n",
    "            temp_ckpt[key[10:]] = ckpt['state_dict'][key]\n",
    "        self.sed_model.load_state_dict(temp_ckpt)\n",
    "        self.sed_model.to(self.device)\n",
    "        self.sed_model.eval()\n",
    "\n",
    "\n",
    "    def predict(self, audiofile):\n",
    "\n",
    "        if audiofile:\n",
    "            waveform, sr = librosa.load(audiofile, sr=32000)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = torch.from_numpy(waveform).float().to(self.device)\n",
    "                output_dict = self.sed_model(x[None, :], None, True)\n",
    "                pred = output_dict['clipwise_output']\n",
    "                pred_post = pred[0].detach().cpu().numpy()\n",
    "                pred_label = np.argmax(pred_post)\n",
    "                pred_prob = np.max(pred_post)\n",
    "            return pred_label, pred_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch = next(iter(audioset_data.train_dataloader()))\n",
    "x = batch[\"waveform\"].to(device)  # shape should be (batch_size, 3, clip_samples)\n",
    "out = sed_model(x)                # no channel‐mismatch error\n",
    "print(\"Output keys:\", out.keys())\n",
    "print(\"Clipwise output shape:\", out[\"clipwise_output\"].shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:43:04.286031600Z",
     "start_time": "2025-03-13T13:42:59.894874200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_11376\\4270949661.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audiocls predict output:  13 8.718129 13\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "Audiocls = Audio_Classification(model_path, config)\n",
    "\n",
    "# pick any audio you like in the ESC-50 testing set (cross-validation)\n",
    "pred_label, pred_prob = Audiocls.predict(\"./workspace/esc-50/raw/ESC-50-master/audio/1-7456-A-13.wav\")\n",
    "\n",
    "print('Audiocls predict output: ', pred_label, pred_prob, gd[\"1-7456-A-13.wav\"])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "htsat_env",
   "language": "python",
   "display_name": "Python (HTSAT_env)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb1a0df39641c41734bdd2d42699ec57167c4cf18fd061cdef52c16cce6262af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
